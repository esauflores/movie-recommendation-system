services:
  # webapp
  postgres-db:
    image: supabase/postgres:15.6.1.113
    container_name: postgres-db
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - 5432:5432
    environment:
      POSTGRES_DB: ${DB_NAME}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_USER: ${DB_USER}
      PGDATA: /var/lib/postgresql/data
    volumes:
      - postgres-data:/var/lib/postgresql/data

  # mlflow
  minio:
    restart: always
    image: minio/minio:latest
    container_name: mlflow-s3
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ':9001' --address ':9000'
    environment:
      - MINIO_ROOT_USER=${AWS_ACCESS_KEY_ID}
      - MINIO_ROOT_PASSWORD=${AWS_SECRET_ACCESS_KEY}
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    container_name: minio-init
    env_file:
      - .env
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set minio http://minio:9000 ${AWS_ACCESS_KEY_ID} ${AWS_SECRET_ACCESS_KEY} &&
      /usr/bin/mc mb minio/mlflow;
      exit 0;
      "

  mlflow-db:
    restart: always
    image: mysql/mysql-server:latest
    container_name: mlflow-db
    ports:
      - "3306:3306"
    environment:
      - MYSQL_DATABASE=${MYSQL_DATABASE}
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD}
    volumes:
      - mlflow-data:/var/lib/mysql

  mlflow:
    restart: always
    build: ./docker/mlflow
    image: mlflow_server
    container_name: mlflow_server
    depends_on:
      - minio-init
      - mlflow-db
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    command: mlflow server --backend-store-uri mysql+pymysql://${MYSQL_USER}:${MYSQL_PASSWORD}@mlflow-db:3306/${MYSQL_DATABASE} --default-artifact-root s3://mlflow/ --host 0.0.0.0

  airflow-db-init:
    image: postgres:13
    container_name: airflow-db-init
    depends_on:
      - postgres-db
    env_file:
      - .env
    environment:
      PGPASSWORD: ${DB_PASSWORD}
    command: >
      bash -c "
      until pg_isready -h postgres-db -p 5432; do sleep 1; done &&
      psql -h postgres-db -U ${DB_USER} -d ${DB_NAME} -c 'CREATE DATABASE airflow_db;' || true"

  airflow-webserver:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-webserver
    restart: always
    depends_on:
      - airflow-db-init
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@postgres-db:5432/airflow_db
      AIRFLOW__CORE__LOAD_EXAMPLES: 'False'
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-scheduler
    restart: always
    depends_on:
      - airflow-webserver
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@postgres-db:5432/airflow_db
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
    command: scheduler

  airflow-init:
    build:
      context: ./airflow
      dockerfile: Dockerfile
    container_name: airflow-init
    depends_on:
      - airflow-db-init
    env_file:
      - .env
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${DB_USER}:${DB_PASSWORD}@postgres-db:5432/airflow_db
    volumes:
      - ./airflow/dags:/opt/airflow/dags
    entrypoint: >
      bash -c "
      airflow db migrate &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com
      "

volumes:
  postgres-data:
  mlflow-data:
  minio-data:
